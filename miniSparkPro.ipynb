{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Ingestion - Load Large Datasets\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkPro\").getOrCreate()\n",
    "\n",
    "# Load large transactions data\n",
    "transactions_df = spark.read.csv(\"large_transactions.csv\")\n",
    "\n",
    "# Load large inventory data\n",
    "inventory_df = spark.read.json(\"large_inventory.json\")\n",
    "\n",
    "# Load large customer feedback data\n",
    "feedback_df = spark.read.csv(\"large_customer_feedback.csv\")\n",
    "\n",
    "# Display a sample of each dataset\n",
    "transactions_df.show(5)\n",
    "inventory_df.show(5)\n",
    "feedback_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning and Transformation with RDDs\n",
    "import hashlib\n",
    "\n",
    "# Convert transactions DataFrame to RDD\n",
    "transactions_rdd = transactions_df.rdd\n",
    "\n",
    "# Filter out corrupted records (e.g., missing transaction_id or amount)\n",
    "cleaned_rdd = transactions_rdd.filter(lambda x: all(field is not None for field in x))\n",
    "\n",
    "\n",
    "# Write Function to Anonymize user IDs using Hashing\n",
    "def anonymize(record):\n",
    "    # Assuming user_id is the second field (index 1 in zero-indexed array)\n",
    "    hashed_id = hashlib.sha256(record[1].encode('utf-8')).hexdigest()\n",
    "    # Recreate the record with anonymized user_id, leaving the rest as is\n",
    "    return (record[0], hashed_id) + tuple(record[2:])\n",
    "\n",
    "anonymized_rdd = cleaned_rdd.map(anonymize)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "original_columns = transactions_df.columns\n",
    "cleaned_transactions_df = anonymized_rdd.toDF(original_columns)\n",
    "\n",
    "# Display cleaned and anonymized data\n",
    "cleaned_transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: DataFrame Operations for Cleaning and Transformation\n",
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "# Clean inventory data by handling missing values and normalizing text\n",
    "cleaned_inventory_df = inventory_df.dropna(subset=[\"stock_level\"]) \\\n",
    "                                  .withColumn(\"product_name\", lower(trim(col(\"product_name\"))))\n",
    "\n",
    "# Display cleaned inventory data\n",
    "cleaned_inventory_df.show(5)\n",
    "\n",
    "# Perform a join operation to combine data\n",
    "joined_df = cleaned_transactions_df.join(\n",
    "    cleaned_inventory_df, cleaned_inventory_df.product_id == cleaned_transactions_df.product_id, \"inner\")\n",
    "\n",
    "# Display joined DataFrame\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Spark SQL Queries\n",
    "from audioop import avg\n",
    "from pyspark.sql.functions import date_format\n",
    "# Create temporary views for SQL queries\n",
    "cleaned_transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "cleaned_inventory_df.createOrReplaceTempView(\"inventory\")\n",
    "joined_df.createOrReplaceTempView(\"joined_data\")\n",
    "\n",
    "# Query: Top 10 most purchased products in the last month\n",
    "top_products_df = spark.sql(\"\"\"\n",
    "    WITH recent_month AS (\n",
    "        SELECT MAX(date_format(transaction_date, 'yyyy-MM')) AS max_month\n",
    "        FROM joined_data\n",
    "    )\n",
    "    SELECT product_name, SUM(quantity) AS total_qty\n",
    "    FROM joined_data\n",
    "    JOIN recent_month\n",
    "    ON date_format(joined_data.transaction_date, 'yyyy-MM') = recent_month.max_month\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_qty DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "top_products_df.show()\n",
    "\n",
    "# Query: Monthly revenue trends\n",
    "monthly_revenue_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(transaction_date, 'yyyy-MM') AS month,\n",
    "        SUM(amount) AS total_revenue\n",
    "    FROM \n",
    "        joined_data\n",
    "    GROUP BY \n",
    "        DATE_FORMAT(transaction_date, 'yyyy-MM')\n",
    "    ORDER BY \n",
    "        month\n",
    "\"\"\")\n",
    "\n",
    "monthly_revenue_df.show()\n",
    "\n",
    "# Query: Inventory turnover rates\n",
    "turnover_rate_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        i.product_name,\n",
    "        SUM(j.quantity) / AVG(i.stock_level) AS turnover_rate\n",
    "    FROM \n",
    "        joined_data j\n",
    "    JOIN \n",
    "        inventory i\n",
    "    ON \n",
    "        j.product_name = i.product_name\n",
    "    GROUP BY \n",
    "        i.product_name\n",
    "    ORDER BY \n",
    "        turnover_rate DESC\n",
    "\"\"\")\n",
    "turnover_rate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Real-Time Processing (Optional)\n",
    "from pyspark.sql.functions import window, countDistinct\n",
    "\n",
    "# For demonstration, create a streaming DataFrame from a sample batch dataset\n",
    "streaming_transactions_df = spark.readStream.schema(transactions_df.schema) \\\n",
    "                                           .csv(\"streaming_transactions_folder\")  # Point to a folder with incoming data\n",
    "\n",
    "# Compute real-time metrics (e.g., active users per minute)\n",
    "active_users = \n",
    "\n",
    "# Display active users in real-time (Note: This will print continuously if run with actual streaming data)\n",
    "query = \n",
    "\n",
    "query.awaitTermination()  # Keep the stream running (can be stopped manually)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Performance Optimization Techniques\n",
    "# Caching DataFrames to optimize performance for multiple transformations\n",
    "cleaned_transactions_df.cache()\n",
    "cleaned_inventory_df.cache()\n",
    "joined_df.cache()\n",
    "\n",
    "# Repartition DataFrames for optimal join performance\n",
    "transactions_df_repartitioned = cleaned_transactions_df.repartition(\"product_id\")\n",
    "inventory_df_repartitioned = cleaned_inventory_df.repartition(\"product_id\")\n",
    "\n",
    "# Use Broadcast Join for small DataFrames (if applicable)\n",
    "joined_df_optimized = transactions_df_repartitioned.join(\n",
    "    cleaned_inventory_df, transactions_df_repartitioned.product_id == cleaned_inventory_df.product_id)\n",
    "\n",
    "# Display the optimized joined DataFrame\n",
    "joined_df_optimized.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dashboards for each of them\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "\n",
    "# top_products_df\n",
    "top_products_pd = top_products_df.toPandas()\n",
    "\n",
    "# Bar plot for top products\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='product_name', y='total_qty', data=top_products_pd)\n",
    "plt.title('Top 10 Most Purchased Products')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#monthly_revenue_df\n",
    "monthly_revenue_pd = monthly_revenue_df.toPandas()\n",
    "\n",
    "# Line plot for monthly revenue\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(x='month', y='total_revenue', data=monthly_revenue_pd, marker=\"o\")\n",
    "plt.title('Monthly Revenue Trend')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#turnover_rate_df\n",
    "turnover_rate_pd = turnover_rate_df.toPandas()\n",
    "\n",
    "# Heatmap for turnover rates (ensure the data is reshaped correctly)\n",
    "plt.figure(figsize=(10,8))\n",
    "heatmap_data = turnover_rate_pd.pivot(\"product_name\", \"turnover_rate\")\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title('Inventory Turnover Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Store the Transformed Data\n",
    "# Store the cleaned and transformed data in Parquet format\n",
    "cleaned_transactions_df.write.parquet(\"cleaned_transactions.parquet\")\n",
    "cleaned_inventory_df.write.parquet(\"cleaned_inventory.parquet\")\n",
    "joined_df.write.parquet(\"joined_df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
